{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NextFlow Documentation","text":"<p>Welcome to Shian's unofficial NextFlow documentation site.</p> <p>Use the navigation menu to explore available topics, including operator documentation and usage guides.</p>"},{"location":"#pipeline","title":"Pipeline","text":"<ul> <li>Configuration</li> </ul>"},{"location":"#proccesses","title":"Proccesses","text":"<ul> <li>Inputs</li> <li>Outputs</li> <li>Directives</li> <li>Properties</li> </ul>"},{"location":"#channels","title":"Channels","text":"<ul> <li>Operators</li> </ul>"},{"location":"Directives/","title":"Process Directives","text":"<p>This page summarizes all process directives available in Nextflow. Directives control process execution, resource allocation, environment, and output handling.</p>"},{"location":"Directives/#resource-management","title":"Resource Management","text":"<p>Directives that control resource allocation for process execution.</p>"},{"location":"Directives/#cpus","title":"cpus","text":"<p>Set number of CPUs.</p> <pre><code>process big_job {\n    cpus 8\n    script:\n    \"\"\"\n    blastp -query input_sequence -num_threads ${task.cpus}\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#memory","title":"memory","text":"<p>Set memory requirement.</p> <pre><code>process big_job {\n    memory '2 GB'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#time","title":"time","text":"<p>Set maximum process runtime.</p> <pre><code>process big_job {\n    time '1h'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#disk","title":"disk","text":"<p>Set local disk storage.</p> <pre><code>process big_job {\n    disk '2 GB'\n    script:\n    \"\"\"\n    your_command\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#accelerator","title":"accelerator","text":"<p>Request hardware accelerators (e.g. GPUs) for a process.</p> <pre><code>process foo {\n    accelerator 4, type: 'nvidia-tesla-k80'\n    script:\n    \"\"\"\n    your_gpu_enabled --command --line\n    \"\"\"\n}\n</code></pre> <p>Only supported by certain executors. See platform docs for details.</p>"},{"location":"Directives/#machinetype","title":"machineType","text":"<p>Specify a cloud machine type.</p> <pre><code>process foo {\n    machineType 'n1-highmem-8'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#resourcelimits","title":"resourceLimits","text":"<p>Set environment-specific resource limits.</p> <pre><code>process my_task {\n    resourceLimits cpus: 24, memory: 768.GB, time: 72.h\n    // ...\n}\n</code></pre>"},{"location":"Directives/#arch","title":"arch","text":"<p>Specify CPU architecture and microarchitecture.</p> <pre><code>process cpu_task {\n    arch 'linux/x86_64', target: 'cascadelake'\n    script:\n    \"\"\"\n    blastp -query input_sequence -num_threads ${task.cpus}\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#execution-control","title":"Execution Control","text":"<p>Directives that affect how and when processes are executed.</p>"},{"location":"Directives/#executor","title":"executor","text":"<p>Override the executor for a process.</p> <pre><code>process doSomething {\n    executor 'sge'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#queue","title":"queue","text":"<p>Set the job queue for grid executors.</p> <pre><code>process grid_job {\n    queue 'long'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#clusteroptions","title":"clusterOptions","text":"<p>Pass native cluster options.</p> <pre><code>process foo {\n    clusterOptions '-x 1 -y 2'\n    // or\n    clusterOptions '-x 1', '-y 2', '--flag'\n}\n</code></pre> <p>Only for grid executors.</p>"},{"location":"Directives/#penv","title":"penv","text":"<p>Set parallel environment for SGE.</p> <pre><code>process big_job {\n    cpus 4\n    penv 'smp'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#array","title":"array","text":"<p>Submit tasks as job arrays (experimental).</p> <pre><code>process cpu_task {\n    executor 'slurm'\n    array 100\n    script:\n    \"\"\"\n    your_command --here\n    \"\"\"\n}\n</code></pre> <p>Supported by AWS Batch, Google Cloud Batch, LSF, PBS, SGE, SLURM.</p>"},{"location":"Directives/#maxforks","title":"maxForks","text":"<p>Limit parallel process instances.</p> <pre><code>process doNotParallelizeIt {\n    maxForks 1\n    // ...\n}\n</code></pre>"},{"location":"Directives/#maxsubmitawait","title":"maxSubmitAwait","text":"<p>Set max time in submission queue.</p> <pre><code>process foo {\n    maxSubmitAwait '10 mins'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#error-handling-retry","title":"Error Handling &amp; Retry","text":"<p>Directives for error management and retry strategies.</p>"},{"location":"Directives/#errorstrategy","title":"errorStrategy","text":"<p>Control error handling.</p> <p><pre><code>process ignoreAnyError {\n    errorStrategy 'ignore'\n    // ...\n}\n</code></pre> - <code>terminate</code> (default) - <code>finish</code> - <code>ignore</code> - <code>retry</code></p>"},{"location":"Directives/#maxretries","title":"maxRetries","text":"<p>Max retries for a failed task.</p> <pre><code>process retryIfFail {\n    errorStrategy 'retry'\n    maxRetries 3\n    // ...\n}\n</code></pre>"},{"location":"Directives/#maxerrors","title":"maxErrors","text":"<p>Max total errors allowed for a process.</p> <pre><code>process retryIfFail {\n    errorStrategy 'retry'\n    maxErrors 5\n    // ...\n}\n</code></pre>"},{"location":"Directives/#environment-dependencies","title":"Environment &amp; Dependencies","text":"<p>Directives for specifying the process environment and dependencies.</p>"},{"location":"Directives/#conda","title":"conda","text":"<p>Define Conda dependencies.</p> <ul> <li>Specify packages: List one or more packages (with optional versions) separated by spaces.</li> <li>Specify channels: Use the <code>channel::package=version</code> syntax to select a channel (e.g., <code>bioconda::bwa=0.7.15</code>).</li> <li>Use environment files: Provide a path to a Conda environment YAML file.</li> <li>Use existing environments: Provide a path to an existing Conda environment directory.</li> </ul> <p>Examples:</p> <pre><code>process foo {\n    // Single package\n    conda 'bwa=0.7.15'\n    script:\n    \"\"\"\n    bwa mem ...\n    \"\"\"\n}\n\nprocess bar {\n    // Multiple packages\n    conda 'bwa=0.7.15 fastqc=0.11.5'\n    script:\n    \"\"\"\n    bwa mem ... &amp;&amp; fastqc ...\n    \"\"\"\n}\n\nprocess baz {\n    // Specify channel\n    conda 'bioconda::bwa=0.7.15'\n    script:\n    \"\"\"\n    bwa mem ...\n    \"\"\"\n}\n\nprocess envFileExample {\n    // Use environment YAML file\n    conda './envs/myenv.yaml'\n    script:\n    \"\"\"\n    my_tool ...\n    \"\"\"\n}\n\nprocess existingEnvExample {\n    // Use existing environment directory\n    conda '/path/to/conda/env'\n    script:\n    \"\"\"\n    my_tool ...\n    \"\"\"\n}\n</code></pre> <p>See Conda environments for more details.</p>"},{"location":"Directives/#spack","title":"spack","text":"<p>Define Spack dependencies.</p> <pre><code>process foo {\n    spack 'bwa@0.7.15'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#module","title":"module","text":"<p>Load environment modules.</p> <pre><code>process basicExample {\n    module 'ncbi-blast/2.2.27'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#container","title":"container","text":"<p>Run the process in a Docker container.</p> <pre><code>process runThisInDocker {\n    container 'dockerbox:tag'\n    script:\n    \"\"\"\n    your_command\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#containeroptions","title":"containerOptions","text":"<p>Extra options for the container engine.</p> <pre><code>process runThisWithDocker {\n    container 'busybox:latest'\n    containerOptions '--volume /data/db:/db'\n    script:\n    \"\"\"\n    your_command --data /db\n    \"\"\"\n}\n</code></pre> <p>Not supported by Kubernetes executor.</p>"},{"location":"Directives/#pod","title":"pod","text":"<p>Kubernetes pod-specific settings.</p> <pre><code>process your_task {\n    pod env: 'FOO', value: 'bar'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#shell","title":"shell","text":"<p>Set a custom shell for the script.</p> <pre><code>process doMoreThings {\n    shell '/bin/bash', '-euo', 'pipefail'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#secret","title":"secret","text":"<p>Expose secrets as environment variables.</p> <pre><code>process someJob {\n    secret 'MY_ACCESS_KEY'\n    secret 'MY_SECRET_KEY'\n    script:\n    \"\"\"\n    your_command --access \\$MY_ACCESS_KEY --secret \\$MY_SECRET_KEY\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#inputoutput-staging","title":"Input/Output Staging","text":"<p>Directives for handling input and output files.</p>"},{"location":"Directives/#stageinmode","title":"stageInMode","text":"<p>Control how input files are staged.</p> <ul> <li><code>'copy'</code></li> <li><code>'link'</code></li> <li><code>'rellink'</code></li> <li><code>'symlink'</code> (default)</li> </ul>"},{"location":"Directives/#stageoutmode","title":"stageOutMode","text":"<p>Control how output files are staged out.</p> <ul> <li><code>'copy'</code></li> <li><code>'fcp'</code></li> <li><code>'move'</code></li> <li><code>'rclone'</code></li> <li><code>'rsync'</code></li> </ul>"},{"location":"Directives/#scratch","title":"scratch","text":"<p>Run process in a temporary local directory.</p> <pre><code>process simpleTask {\n    scratch true\n    output:\n    path 'data_out'\n    script:\n    \"\"\"\n    your_command\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#output-publishing-caching","title":"Output Publishing &amp; Caching","text":"<p>Directives for publishing and caching process outputs.</p>"},{"location":"Directives/#publishdir","title":"publishDir","text":"<p>Publish output files to a directory. The <code>publishDir</code> directive allows you to automatically copy, move, or link process output files to a specified directory or remote storage (e.g., S3 bucket). This is useful for collecting results in a single location for downstream analysis or sharing.</p> <pre><code>process foo {\n    publishDir '/data/chunks', mode: 'copy', overwrite: false\n    output:\n    path 'chunk_*'\n    script:\n    \"\"\"\n    printf 'Hola' | split -b 1 - chunk_\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#available-options","title":"Available Options","text":"<p>You can specify options as a map, e.g. <code>publishDir path: '/some/dir', mode: 'copy', overwrite: true</code>.</p> Option Description <code>path</code> Directory or remote location where files are published. Shortcut: <code>publishDir '/some/dir'</code> is equivalent to <code>publishDir path: '/some/dir'</code>. <code>mode</code> File publishing method: - <code>'copy'</code>: Copy files to publish directory. - <code>'copyNoFollow'</code>: Copy files without following symlinks. - <code>'link'</code>: Create hard links. - <code>'move'</code>: Move files (use only for terminal processes). - <code>'rellink'</code>: Create relative symlinks. - <code>'symlink'</code>: Create absolute symlinks (default). <code>overwrite</code> Overwrite existing files in the target directory. Default: <code>true</code> during normal execution, <code>false</code> when resuming. <code>pattern</code> Glob pattern to select which output files to publish. <code>saveAs</code> Closure to rename or change the destination path of published files. Return <code>null</code> to skip publishing a file. Useful for dynamic naming or selective publishing.Example:<code>&lt;br&gt;publishDir '/results', saveAs: { filename -&gt; filename.endsWith('.txt') ? \"renamed_${filename}\" : null }&lt;br&gt;</code> <code>enabled</code> Enable or disable publishing for this rule. Default: <code>true</code>.Example: <code>enabled: params.publish_results</code> <code>failOnError</code> Abort execution if publishing fails for any file. Default: <code>true</code> (since v24.03.0-edge; was <code>false</code> before). <code>contentType</code> (Experimental, S3 only)Specify the media (MIME) type of the published file. If set to <code>true</code>, inferred from file extension. Default: <code>false</code>. <code>storageClass</code> (Experimental, S3 only)Specify the storage class for the published file (e.g., <code>STANDARD</code>, <code>GLACIER</code>). <code>tags</code> (Experimental, S3 only)Associate arbitrary tags with the published file.Example: <code>tags: [FOO: 'Hello world']</code>"},{"location":"Directives/#examples","title":"Examples","text":"<p>Basic usage: <pre><code>process foo {\n    publishDir '/results'\n    output:\n    path 'output.txt'\n    script:\n    \"\"\"\n    echo result &gt; output.txt\n    \"\"\"\n}\n</code></pre></p> <p>Publish only <code>.txt</code> files and rename them: <pre><code>process bar {\n    publishDir '/results', pattern: '*.txt', saveAs: { name -&gt; \"renamed_${name}\" }\n    output:\n    path '*.txt'\n    path '*.log'\n    script:\n    \"\"\"\n    echo foo &gt; foo.txt\n    echo bar &gt; bar.log\n    \"\"\"\n}\n</code></pre></p> <p>Publish to S3 with custom storage class and tags: <pre><code>process s3_publish {\n    publishDir 's3://my-bucket/results', storageClass: 'STANDARD_IA', tags: [project: 'nf-demo']\n    output:\n    path 'result.txt'\n    script:\n    \"\"\"\n    echo data &gt; result.txt\n    \"\"\"\n}\n</code></pre></p> <p>Disable publishing conditionally: <pre><code>process conditional_publish {\n    publishDir '/results', enabled: params.publish_results\n    output:\n    path 'output.txt'\n    script:\n    \"\"\"\n    echo something &gt; output.txt\n    \"\"\"\n}\n</code></pre></p> <p>Fail pipeline if publishing fails: <pre><code>process strict_publish {\n    publishDir '/results', failOnError: true\n    output:\n    path 'output.txt'\n    script:\n    \"\"\"\n    echo fail &gt; output.txt\n    \"\"\"\n}\n</code></pre></p> <p>For more details, see the Nextflow documentation on publishDir.</p>"},{"location":"Directives/#storedir","title":"storeDir","text":"<p>Permanent cache for process results.</p> <pre><code>process formatBlastDatabases {\n    storeDir '/db/genomes'\n    // ...\n}\n</code></pre>"},{"location":"Directives/#cache","title":"cache","text":"<p>Control process result caching.</p> <p><pre><code>process noCacheThis {\n    cache false\n    // ...\n}\n</code></pre> - <code>false</code>: Disable caching - <code>true</code>: Enable (default) - <code>'deep'</code>: Use file content - <code>'lenient'</code>: Minimal metadata</p>"},{"location":"Directives/#metadata-customization","title":"Metadata &amp; Customization","text":"<p>Directives for process labeling, tagging, and custom metadata.</p>"},{"location":"Directives/#label","title":"label","text":"<p>Annotate processes for grouping/configuration.</p> <ul> <li>Purpose: Use <code>label</code> to assign one or more static labels to a process. These labels are mainly used for configuration and resource selection in your <code>nextflow.config</code> file (e.g., to apply settings to all processes with a given label).</li> <li>Scope: Labels are static and set at the process definition level.</li> </ul> <pre><code>process bigTask {\n    label 'big_mem'\n    // ...\n}\n</code></pre> <p>Example: Use label for configuration <pre><code>// In nextflow.config\nprocess {\n    withLabel: big_mem {\n        cpus = 16\n        memory = '64 GB'\n    }\n}\n</code></pre></p>"},{"location":"Directives/#tag","title":"tag","text":"<p>Custom label for each process execution.</p> <ul> <li>Purpose: Use <code>tag</code> to dynamically assign a string to each process execution (task). This is useful for tracking, logging, and making trace files more informative.</li> <li>Scope: Tags can use process properties and are evaluated per task.</li> </ul> <pre><code>process foo {\n    tag \"$foo\"\n    input:\n    val foo\n    script:\n    \"\"\"\n    echo $foo\n    \"\"\"\n}\n</code></pre> <p>How tag influences job names (e.g. SLURM):</p> <p>When using cluster executors like SLURM, the <code>tag</code> value is often included in the job name submitted to the scheduler. By default, Nextflow sets the job name to include the process name and, if a tag is specified, the tag value. This makes it easier to identify jobs in the cluster queue.</p> <p>Example: If you set: <pre><code>process foo {\n    tag { \"SAMPLE_${sample_id}\" }\n    input:\n    val sample_id\n    script:\n    \"\"\"\n    echo \"Processing $sample_id\"\n    \"\"\"\n}\n</code></pre> The SLURM job name will include <code>foo (SAMPLE_sample1)</code> for a task with <code>sample_id = 'sample1'</code>.</p> <p>You can further customize the job name using the <code>executor.jobName</code> config option: <pre><code>executor {\n    jobName = { \"${task.process} ${task.tag}\" }\n}\n</code></pre></p> <p>Example: Use tag with process properties <pre><code>process foo {\n    tag { \"sample:${task.id} (${task.process})\" }\n    input:\n    val sample_id\n    script:\n    \"\"\"\n    echo \"Processing sample $sample_id\"\n    \"\"\"\n}\n</code></pre> - Here, <code>task.id</code> and <code>task.process</code> are process properties available for use in the tag.</p> <p>Common process properties for tag:</p> <ul> <li><code>task.id</code>: Unique pipeline task index</li> <li><code>task.index</code>: Process-level task index</li> <li><code>task.process</code>: Process name</li> <li><code>task.attempt</code>: Current retry attempt</li> <li><code>task.cpus</code>, <code>task.memory</code>: Allocated resources</li> </ul> <p>Summary Table</p> Directive Purpose Scope Example Usage <code>label</code> Static grouping/configuration Process definition Resource selection in config <code>tag</code> Dynamic per-task annotation Each task Logging, trace, reporting"},{"location":"Directives/#script-hooks","title":"Script Hooks","text":"<p>Directives for running code before or after the main script.</p>"},{"location":"Directives/#beforescript","title":"beforeScript","text":"<p>Run a Bash snippet before the main process script.</p> <pre><code>process foo {\n    beforeScript 'source /cluster/bin/setup'\n    script:\n    \"\"\"\n    echo bar\n    \"\"\"\n}\n</code></pre>"},{"location":"Directives/#afterscript","title":"afterScript","text":"<p>Run a Bash snippet immediately after the main process script.</p> <pre><code>process foo {\n    afterScript 'rm -rf temp_dir'\n    script:\n    \"\"\"\n    your_command\n    \"\"\"\n}\n</code></pre> <p>Runs outside the container if <code>container</code> is used.</p>"},{"location":"Directives/#miscellaneous","title":"Miscellaneous","text":""},{"location":"Directives/#debug","title":"debug","text":"<p>Show process stdout in the terminal.</p> <pre><code>process sayHello {\n    debug true\n    script:\n    \"\"\"\n    echo Hello\n    \"\"\"\n}\n</code></pre>"},{"location":"Inputs/","title":"Inputs","text":"<p>This page summarizes the different input types available for Nextflow processes. Use these declarations to specify how data is provided to your process scripts.</p>"},{"location":"Inputs/#val","title":"val","text":"<p>Declare a variable input. The received value can be any type and is made available to the process body as a variable with the given identifier.</p> <ul> <li>Syntax: <code>val(identifier)</code></li> <li>Input: Any value.</li> <li>Usage: <pre><code>process example {\n    input:\n    val(x)\n    script:\n    \"\"\"\n    echo $x\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Inputs/#path","title":"path","text":"<p>Declare a file input. The value should be a file or collection of files and is staged into the task directory.</p> <ul> <li>Syntax: <code>path(identifier | stageName)</code></li> <li>Input: File or collection of files.</li> <li>Options:<ul> <li><code>arity</code>: Number or range of expected files (e.g., <code>1</code>, <code>1..*</code>). Task fails if invalid.</li> <li><code>name</code> / <code>stageAs</code>: Name or pattern for the file(s) in the task directory.</li> </ul> </li> <li>Usage: <pre><code>process example {\n    input:\n    path('input.txt', stageAs: 'renamed.txt')\n    script:\n    \"\"\"\n    cat renamed.txt\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Inputs/#env","title":"env","text":"<p>Declare an environment variable input. The value should be a string and is exported to the task environment as the given variable name.</p> <ul> <li>Syntax: <code>env(name)</code></li> <li>Input: String.</li> <li>Usage: <pre><code>process example {\n    input:\n    env(MY_VAR)\n    script:\n    \"\"\"\n    echo \\$MY_VAR\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Inputs/#stdin","title":"stdin","text":"<p>Declare a stdin input. The value should be a string and is provided as standard input to the task script. Only one stdin input can be declared per process.</p> <ul> <li>Syntax: <code>stdin</code></li> <li>Input: String.</li> <li>Usage: <pre><code>process example {\n    input:\n    stdin\n    script:\n    \"\"\"\n    cat\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Inputs/#tuple","title":"tuple","text":"<p>Declare a tuple input. Each argument should be an input declaration (<code>val</code>, <code>path</code>, <code>env</code>, or <code>stdin</code>). The received value should be a tuple matching the declaration.</p> <ul> <li>Syntax: <code>tuple(arg1, arg2, ...)</code></li> <li>Input: Tuple with elements matching the declarations.</li> <li>Usage: <pre><code>process example {\n    input:\n    tuple(val(x), path(y))\n    script:\n    \"\"\"\n    echo $x\n    cat $y\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Operators/","title":"Operators","text":""},{"location":"Operators/#transformation-operators","title":"Transformation Operators","text":"<p>These operators apply functions to each item in a channel, producing a new channel with transformed data. Use them to modify, aggregate, or restructure data as it flows through your pipeline.</p>"},{"location":"Operators/#map","title":"map","text":"<p>Use <code>map</code> to transform each item in a channel, such as converting formats, scaling values, or extracting fields.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting the result of applying a function to each input item.</li> <li>Arguments:<ul> <li><code>closure</code> (required): The transformation function to apply to each item.</li> </ul> </li> </ul> <p><pre><code>Channel.of(1, 2, 3)\n    .map { it * 2 }\n    .view() // Output: 2, 4, 6\n</code></pre> See also: <code>flatMap</code>. While <code>map</code> transforms each item individually, <code>flatMap</code> can emit multiple items for each input by flattening collections. Example (difference from <code>flatMap</code>): <pre><code>// map emits a list as a single item\nChannel.of([1, 2], [3, 4])\n    .map { it }\n    .view() // Output: [1, 2], [3, 4]\n// flatMap emits each element of the list as a separate item\nChannel.of([1, 2], [3, 4])\n    .flatMap { it }\n    .view() // Output: 1, 2, 3, 4\n</code></pre></p>"},{"location":"Operators/#flatmap","title":"flatMap","text":"<p>Use <code>flatMap</code> to expand nested lists or split items into multiple outputs for further processing.</p> <ul> <li>Input: A channel emitting collections or items that can be expanded.</li> <li>Output: A channel emitting each element from the collections, flattened into a single stream.</li> <li>Arguments:<ul> <li><code>closure</code> (required): The transformation function returning a collection for each item.</li> </ul> </li> </ul> <p><pre><code>Channel.of([1, 2], [3, 4])\n    .flatMap { it }\n    .view() // Output: 1, 2, 3, 4\n</code></pre> See also: <code>map</code>. Use <code>flatMap</code> when your transformation returns collections and you want to emit their elements individually, unlike <code>map</code> which emits the collection as a single item. Example (difference from <code>map</code>): <pre><code>// flatMap emits each element of the list as a separate item\nChannel.of([1, 2], [3, 4])\n    .flatMap { it }\n    .view() // Output: 1, 2, 3, 4\n// map emits a list as a single item\nChannel.of([1, 2], [3, 4])\n    .map { it }\n    .view() // Output: [1, 2], [3, 4]\n</code></pre></p>"},{"location":"Operators/#collect","title":"collect","text":"<p>Use <code>collect</code> to gather all items into a single collection for summary operations or final output.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting a single list containing all input items.</li> <li>Arguments:<ul> <li><code>closure</code> (optional): Custom aggregation logic. If omitted, collects items into a list.</li> </ul> </li> </ul> <p><pre><code>Channel.of(1, 2, 3)\n    .collect()\n    .view() // Output: [1, 2, 3]\n</code></pre> See also: <code>toList</code>. Both aggregate all items into a list, but <code>collect</code> is more general and can be customized for other types of aggregation. Example (difference from <code>toList</code>): <pre><code>// collect can be customized for other aggregations\nChannel.of(1, 2, 3)\n    .collect { acc, val -&gt; acc + val * 2 }\n    .view() // Output: [2, 4, 6]\n// toList always collects all items into a list\nChannel.of(1, 2, 3)\n    .toList()\n    .view() // Output: [1, 2, 3]\n</code></pre></p>"},{"location":"Operators/#flatten","title":"flatten","text":"<p>Use <code>flatten</code> to remove one level of nesting from channel items, useful after aggregation or grouping.</p> <ul> <li>Input: A channel emitting nested collections.</li> <li>Output: A channel emitting all elements from the nested collections as a flat stream.</li> <li>Arguments: None.</li> </ul> <p><pre><code>Channel.of([1, 2], [3, 4])\n    .flatten()\n    .view() // Output: 1, 2, 3, 4\n</code></pre> See also: <code>flatMap</code>. Both flatten nested collections, but <code>flatMap</code> applies a transformation before flattening, while <code>flatten</code> only removes one level of nesting. Example (difference from <code>flatMap</code>): <pre><code>// flatten removes one level of nesting, but does not apply a transformation\nChannel.of([1, 2], [3, 4])\n    .flatten()\n    .view() // Output: 1, 2, 3, 4\n// flatMap applies a transformation and flattens\nChannel.of([1, 2], [3, 4])\n    .flatMap { it }\n    .view() // Output: 1, 2, 3, 4\n</code></pre></p>"},{"location":"Operators/#tolist","title":"toList","text":"<p>Use <code>toList</code> to collect all items for batch processing or output as a single list.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting a single list of all items, similar to <code>collect</code>.</li> <li>Arguments: None.</li> </ul> <p><pre><code>Channel.of('a', 'b', 'c')\n    .toList()\n    .view() // Output: ['a', 'b', 'c']\n</code></pre> See also: <code>collect</code>. Both gather all items into a list, but <code>toList</code> is a convenience method for this specific case. Example (difference from <code>collect</code>): <pre><code>// toList always collects all items into a list\nChannel.of(1, 2, 3)\n    .toList()\n    .view() // Output: [1, 2, 3]\n// collect can be customized for other aggregations\nChannel.of(1, 2, 3)\n    .collect { acc, val -&gt; acc + val * 2 }\n    .view() // Output: [2, 4, 6]\n</code></pre></p>"},{"location":"Operators/#tosortedlist","title":"toSortedList","text":"<p>Use <code>toSortedList</code> to collect and sort all items, such as for reporting or ordered output.</p> <ul> <li>Input: A channel emitting comparable items.</li> <li>Output: A channel emitting a single sorted list of all items.</li> <li>Arguments:<ul> <li><code>closure</code> (optional): Custom comparator for sorting.</li> </ul> </li> </ul> <p><pre><code>Channel.of(3, 1, 2)\n    .toSortedList()\n    .view() // Output: [1, 2, 3]\n</code></pre> See also: <code>toList</code>. <code>toSortedList</code> sorts the items, while <code>toList</code> preserves their original order. Example (difference from <code>toList</code>): <pre><code>// toSortedList sorts the items\nChannel.of(3, 1, 2)\n    .toSortedList()\n    .view() // Output: [1, 2, 3]\n// toList preserves order\nChannel.of(3, 1, 2)\n    .toList()\n    .view() // Output: [3, 1, 2]\n</code></pre></p>"},{"location":"Operators/#transpose","title":"transpose","text":"<p>Use <code>transpose</code> to flatten nested lists in tuples, emitting a new tuple for each element in the nested lists.</p> <ul> <li>Input: A channel emitting tuples or lists, where at least one element is a list.</li> <li>Output: A channel emitting tuples with the nested lists expanded into separate items.</li> <li>Arguments:<ul> <li><code>by</code> (optional): Index or list of indices to transpose (default: all list elements).</li> <li><code>remainder</code> (optional): If true, emits incomplete tuples with <code>null</code> for missing elements.</li> </ul> </li> </ul> <p><pre><code>Channel.of(\n    [1, ['A', 'B', 'C']],\n    [2, ['C', 'A']],\n    [3, ['B', 'D']]\n)\n.transpose()\n.view()\n// Output:\n// [1, A]\n// [1, B]\n// [1, C]\n// [2, C]\n// [2, A]\n// [3, B]\n// [3, D]\n</code></pre> See also: <code>groupTuple</code>. While <code>groupTuple</code> groups by key, <code>transpose</code> expands nested lists into separate tuples. Example (with remainder): <pre><code>Channel.of(\n    [1, [1], ['A']],\n    [2, [1, 2], ['B', 'C']],\n    [3, [1, 2, 3], ['D', 'E']]\n)\n.transpose(remainder: true)\n.view()\n// Output:\n// [1, 1, A]\n// [2, 1, B]\n// [2, 2, C]\n// [3, 1, D]\n// [3, 2, E]\n// [3, 3, null]\n</code></pre></p>"},{"location":"Operators/#filtering-operators","title":"Filtering Operators","text":"<p>These operators filter items in a channel based on specified conditions. Use them to select, limit, or exclude data as it flows through your pipeline.</p>"},{"location":"Operators/#filter","title":"filter","text":"<p>Use <code>filter</code> to select items that meet specific criteria, such as filtering by value, pattern, or type.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting only items that match the given predicate (closure, regex, or type).</li> <li>Arguments:<ul> <li><code>closure</code> (optional): Predicate function.</li> <li><code>regex</code> (optional): Regular expression for string matching.</li> <li><code>type</code> (optional): Class type for filtering by type.</li> </ul> </li> </ul> <p><pre><code>// By closure\nChannel.of(1, 2, 3, 4)\n    .filter { it % 2 == 0 }\n    .view() // Output: 2, 4\n// By regex\nChannel.of('apple', 'banana', 'apricot')\n    .filter(/^a.*/)\n    .view() // Output: apple, apricot\n// By type\nChannel.of(1, 'a', 2, 'b')\n    .filter(Number)\n    .view() // Output: 1, 2\n</code></pre> See also: <code>take</code>, <code>skip</code>. While <code>filter</code> selects items based on a condition, <code>take</code> and <code>skip</code> select items based on their position in the stream. Example (difference from <code>take</code> and <code>skip</code>): <pre><code>// filter selects items by condition\nChannel.of(1, 2, 3, 4)\n    .filter { it &gt; 2 }\n    .view() // Output: 3, 4\n// take selects by position\nChannel.of(1, 2, 3, 4)\n    .take(2)\n    .view() // Output: 1, 2\n// skip ignores by position\nChannel.of(1, 2, 3, 4)\n    .skip(2)\n    .view() // Output: 3, 4\n</code></pre></p>"},{"location":"Operators/#take","title":"take","text":"<p>Use <code>take</code> to limit the number of items processed downstream, such as for sampling or testing.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting only the first N items.</li> <li>Arguments:<ul> <li><code>n</code> (required): Number of items to emit.</li> </ul> </li> </ul> <p><pre><code>Channel.of('a', 'b', 'c', 'd')\n    .take(2)\n    .view() // Output: 'a', 'b'\n</code></pre> See also: <code>skip</code>, <code>filter</code>. <code>take</code> emits the first N items, while <code>skip</code> ignores the first N. Use <code>filter</code> for condition-based selection. Example (difference from <code>filter</code> and <code>skip</code>): <pre><code>// take emits the first N items\nChannel.of('a', 'b', 'c', 'd')\n    .take(2)\n    .view() // Output: 'a', 'b'\n// filter emits items matching a condition\nChannel.of('a', 'b', 'c', 'd')\n    .filter { it &gt; 'b' }\n    .view() // Output: 'c', 'd'\n// skip ignores the first N items\nChannel.of('a', 'b', 'c', 'd')\n    .skip(2)\n    .view() // Output: 'c', 'd'\n</code></pre></p>"},{"location":"Operators/#skip","title":"skip","text":"<p>Use <code>skip</code> to ignore a fixed number of initial items, such as skipping headers or warm-up data.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting all items except the first N.</li> <li>Arguments:<ul> <li><code>n</code> (required): Number of items to skip.</li> </ul> </li> </ul> <p><pre><code>Channel.of(10, 20, 30, 40)\n    .skip(2)\n    .view() // Output: 30, 40\n</code></pre> See also: <code>take</code>. <code>skip</code> ignores the first N items, while <code>take</code> emits only the first N. Example (difference from <code>take</code>): <pre><code>// skip ignores the first N items\nChannel.of(10, 20, 30, 40)\n    .skip(2)\n    .view() // Output: 30, 40\n// take emits only the first N items\nChannel.of(10, 20, 30, 40)\n    .take(2)\n    .view() // Output: 10, 20\n</code></pre></p>"},{"location":"Operators/#combining-operators","title":"Combining Operators","text":"<p>These operators combine data from multiple channels. Use them to merge, join, or relate data streams for complex workflows.</p>"},{"location":"Operators/#mix","title":"mix","text":"<p>Use <code>mix</code> to merge multiple data sources into a single stream for unified processing.</p> <ul> <li>Input: Two or more channels emitting any type of item.</li> <li>Output: A single channel emitting all items from the input channels, in the order they become available.</li> <li>Arguments:<ul> <li><code>channels</code> (required): Two or more channels to merge.</li> </ul> </li> </ul> <p><pre><code>Channel.of(1, 2)\n    .mix(Channel.of(3, 4))\n    .view() // Output: 1, 2, 3, 4 (order may vary)\n</code></pre> See also: <code>concat</code>. Both merge multiple channels, but <code>mix</code> emits items as soon as they are available from any input, while <code>concat</code> emits all items from one channel before moving to the next. Example (difference from <code>concat</code>): <pre><code>// mix interleaves items as they arrive\nChannel.of(1, 2)\n    .mix(Channel.of(10, 20))\n    .view() // Output: 1, 10, 2, 20 (order may vary)\n// concat preserves channel order\nChannel.of(1, 2)\n    .concat(Channel.of(10, 20))\n    .view() // Output: 1, 2, 10, 20\n</code></pre></p>"},{"location":"Operators/#concat","title":"concat","text":"<p>Use <code>concat</code> to chain channels together in a specific order, such as appending results or logs.</p> <ul> <li>Input: Two or more channels emitting any type of item.</li> <li>Output: A single channel emitting all items from the first channel, then all items from the next, and so on.</li> <li>Arguments:<ul> <li><code>channels</code> (required): Two or more channels to concatenate.</li> </ul> </li> </ul> <p><pre><code>Channel.of('a', 'b')\n    .concat(Channel.of('c', 'd'))\n    .view() // Output: 'a', 'b', 'c', 'd'\n</code></pre> See also: <code>mix</code>. Use <code>concat</code> when you need to preserve the order of channels, unlike <code>mix</code> which interleaves items as they arrive. Example (difference from <code>mix</code>): <pre><code>// concat emits all items from the first channel, then the next\nChannel.of('a', 'b')\n    .concat(Channel.of('x', 'y'))\n    .view() // Output: 'a', 'b', 'x', 'y'\n// mix interleaves items as they become available\nChannel.of('a', 'b')\n    .mix(Channel.of('x', 'y'))\n    .view() // Output: 'a', 'x', 'b', 'y' (order may vary)\n</code></pre></p>"},{"location":"Operators/#combine","title":"combine","text":"<p>Use <code>combine</code> to relate or synchronize data from two sources based on a common identifier.</p> <ul> <li>Input: Two channels emitting tuples or lists, with a key to match items.</li> <li>Output: A channel emitting pairs of items from both channels that share the same key.</li> <li>Arguments:<ul> <li><code>other</code> (required): The other channel to combine with.</li> <li><code>by</code> (required): Index or closure to specify the key for matching.</li> </ul> </li> </ul> <p><pre><code>Channel.of([1, 'A'], [2, 'B'])\n    .combine(Channel.of([1, 10], [2, 20]), by: 0)\n    .view() // Output: [[1, 'A'], [1, 10]], [[2, 'B'], [2, 20]]\n</code></pre> See also: <code>join</code>. Both relate items from two channels by key, but <code>combine</code> emits all possible pairs for matching keys, while <code>join</code> emits a single joined tuple per key. Example (difference from <code>join</code>): <pre><code>// combine emits all pairs for matching keys\nChannel.of([1, 'A'], [1, 'B'])\n    .combine(Channel.of([1, 10], [1, 20]), by: 0)\n    .view() // Output: [[1, 'A'], [1, 10]], [[1, 'A'], [1, 20]], [[1, 'B'], [1, 10]], [[1, 'B'], [1, 20]]\n// join emits a single joined tuple per key\nChannel.of([1, 'A'], [1, 'B'])\n    .join(Channel.of([1, 10], [1, 20]), by: 0)\n    .view() // Output: [1, 'A', 10], [1, 'B', 20]\n</code></pre></p>"},{"location":"Operators/#join","title":"join","text":"<p>Use <code>join</code> to enrich or correlate data from two sources, such as joining metadata with results.</p> <ul> <li>Input: Two channels emitting tuples or lists, with a key to match items.</li> <li>Output: A channel emitting joined tuples for each matching key, similar to a database join.</li> <li>Arguments:<ul> <li><code>other</code> (required): The other channel to join with.</li> <li><code>by</code> (required): Index or closure to specify the key for matching.</li> </ul> </li> </ul> <p><pre><code>Channel.of([1, 'foo'], [2, 'bar'])\n    .join(Channel.of([1, 30], [2, 40]), by: 0)\n    .view() // Output: [1, 'foo', 30], [2, 'bar', 40]\n</code></pre> See also: <code>combine</code>. Use <code>join</code> for a database-style join (one output per key), while <code>combine</code> can emit multiple pairs for each key. Example (difference from <code>combine</code>): <pre><code>// join emits one joined tuple per key\nChannel.of([2, 'foo'], [2, 'bar'])\n    .join(Channel.of([2, 100], [2, 200]), by: 0)\n    .view() // Output: [2, 'foo', 100], [2, 'bar', 200]\n// combine emits all possible pairs for matching keys\nChannel.of([2, 'foo'], [2, 'bar'])\n    .combine(Channel.of([2, 100], [2, 200]), by: 0)\n    .view() // Output: [[2, 'foo'], [2, 100]], [[2, 'foo'], [2, 200]], [[2, 'bar'], [2, 100]], [[2, 'bar'], [2, 200]]\n</code></pre></p>"},{"location":"Operators/#cross","title":"cross","text":"<p>Use <code>cross</code> to generate all combinations of two datasets, such as parameter sweeps or matrix builds.</p> <ul> <li>Input: Two channels emitting any type of item.</li> <li>Output: A channel emitting the Cartesian product (all possible pairs) of items from both channels.</li> <li>Arguments:<ul> <li><code>other</code> (required): The other channel to cross with.</li> </ul> </li> </ul> <p><pre><code>Channel.of([1, 'A'], [2, 'B'])\n    .cross(Channel.of([1, 10], [2, 20]))\n    .view() // Output: [[1, 'A'], [1, 10]], [[2, 'B'], [2, 20]]\n</code></pre> See also: <code>combine</code>. <code>cross</code> produces the full Cartesian product of two channels, while <code>combine</code> only pairs items with matching keys. Example (difference from <code>combine</code>): <pre><code>// cross produces all possible pairs (Cartesian product)\nChannel.of(1, 2)\n    .cross(Channel.of('a', 'b'))\n    .view() // Output: [1, 'a'], [1, 'b'], [2, 'a'], [2, 'b']\n// combine only pairs items with matching keys\nChannel.of([1, 'A'], [2, 'B'])\n    .combine(Channel.of([1, 10], [2, 20]), by: 0)\n    .view() // Output: [[1, 'A'], [1, 10]], [[2, 'B'], [2, 20]]\n</code></pre></p>"},{"location":"Operators/#grouping-and-batching-operators","title":"Grouping and Batching Operators","text":"<p>These operators group or batch items from a channel. Use them to organize data into manageable sets for downstream processing or aggregation.</p>"},{"location":"Operators/#buffer","title":"buffer","text":"<p>Use <code>buffer</code> to batch items for grouped processing, such as running jobs in chunks or controlling resource usage.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting lists of items, each list containing up to the specified batch size or meeting a condition.</li> <li>Arguments:<ul> <li><code>size</code> (optional): Maximum number of items per batch.</li> <li><code>timeout</code> (optional): Maximum time (in milliseconds or as a duration string, e.g. <code>'5s'</code>) to wait before emitting a batch.</li> <li><code>remainder</code> (optional): If <code>true</code>, emit any remaining items as a partial batch at the end (default: <code>false</code>).</li> <li><code>skip</code> (optional): Number of items to skip before starting the next batch (for sliding windows).</li> <li><code>openingCondition</code> (optional): Start a new batch when this condition is met (can be a value, regex, type, or closure).</li> <li><code>closingCondition</code> (optional): Emit the batch when this condition is met (can be a value, regex, type, or closure).</li> </ul> </li> </ul> <p><pre><code>Channel.of(1, 2, 3, 4, 5)\n    .buffer(size: 2)\n    .view() // Output: [1, 2], [3, 4], [5]\n</code></pre> See also: <code>collate</code>. <code>buffer</code> is more flexible and supports batching by size, time, or custom conditions.</p>"},{"location":"Operators/#collate","title":"collate","text":"<p>Use <code>collate</code> to create sliding windows or overlapping groups for analysis or rolling computations.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting lists of a fixed size, optionally with overlap between groups.</li> <li>Arguments:<ul> <li><code>size</code> (required): Number of items per group.</li> <li><code>step</code> (optional): Number of items to advance between groups (default: same as <code>size</code>).</li> <li><code>remainder</code> (optional): If <code>true</code>, emit any remaining items as a partial group at the end (default: <code>true</code>).</li> </ul> </li> </ul> <p><pre><code>Channel.of(1, 2, 3, 4)\n    .collate(3, 1)\n    .view() // Output: [1, 2, 3], [2, 3, 4], [3, 4], [4]\n</code></pre> See also: <code>buffer</code>. <code>collate</code> is a simpler batching operator for fixed-size groups and sliding windows.</p>"},{"location":"Operators/#grouptuple","title":"groupTuple","text":"<p>Use <code>groupTuple</code> to group related data by a common identifier, such as grouping results by sample or category.</p> <ul> <li>Input: A channel emitting tuples or lists with a key field.</li> <li>Output: A channel emitting pairs of [key, grouped items] where items share the same key.</li> <li>Arguments:<ul> <li><code>by</code> (optional): Index or list of indices to use as the grouping key (default: <code>0</code>).</li> <li><code>size</code> (optional): Number of items expected per group. If set, groups are emitted as soon as they reach this size.</li> <li><code>remainder</code> (optional): If <code>true</code>, incomplete groups (with fewer than <code>size</code> items) are emitted at the end (default: <code>false</code>).</li> <li><code>sort</code> (optional): Sorting criteria for grouped items. Can be <code>false</code> (no sort, default), <code>true</code> (natural order), <code>'hash'</code>, <code>'deep'</code>, or a custom closure/comparator.</li> </ul> </li> </ul> <p><pre><code>Channel.of([1, 'A'], [1, 'B'], [2, 'C'])\n    .groupTuple(by: 0)\n    .view() // Output: [1, [[1, 'A'], [1, 'B']]], [2, [[2, 'C']]]\n</code></pre> See also: <code>transpose</code>. <code>groupTuple</code> groups by key, while <code>transpose</code> expands nested lists in tuples.</p>"},{"location":"Operators/#splitting-and-parsing-operators","title":"Splitting and Parsing Operators","text":"<p>These operators split or parse structured data from files or strings into records or chunks for downstream processing.</p>"},{"location":"Operators/#splittext","title":"splitText","text":"<p>Use <code>splitText</code> to split multi-line text into lines or chunks of lines.</p> <p>Example input file (<code>input.txt</code>): <pre><code>a\nb\nc\nd\ne\n</code></pre></p> <ul> <li>Input: A channel emitting files or multi-line strings.</li> <li>Output: A channel emitting lines or chunks of lines.</li> <li>Arguments:<ul> <li><code>by</code> (optional): Number of lines per chunk (default: 1).</li> </ul> </li> </ul> <p><pre><code>Channel.fromPath('input.txt')\n    .splitText()\n    .view() // Output: 'a', 'b', 'c', 'd', 'e'\nChannel.fromPath('input.txt')\n    .splitText(by: 2)\n    .view() // Output: 'a\\nb\\n', 'c\\nd\\n', 'e\\n'\n</code></pre> See also: <code>splitCsv</code>, <code>splitFasta</code>.</p>"},{"location":"Operators/#splitcsv","title":"splitCsv","text":"<p>Use <code>splitCsv</code> to parse CSV-formatted text into rows (as lists or maps).</p> <p>Arguments: - <code>by</code>: When specified, group rows into chunks with the given size (default: none). - <code>charset</code>: Parse the content with the specified charset, e.g. UTF-8. See the list of standard charsets for available options. - <code>decompress</code>: When true, decompress the content using the GZIP format before processing it (default: false). Files with the <code>.gz</code> extension are decompressed automatically. - <code>elem</code>: The index of the element to split when the source items are lists or tuples (default: first file object or first element). - <code>header</code>: When true, the first line is used as the columns names (default: false). Can also be a list of columns names. - <code>limit</code>: Limits the number of records to retrieve for each source item (default: no limit). - <code>quote</code>: The character used to quote values (default: '' or \"\"). - <code>sep</code>: The character used to separate values (default: <code>,</code>). - <code>skip</code>: Number of lines to ignore from the beginning when parsing the CSV text (default: 0). - <code>strip</code>: When true, remove leading and trailing blanks from values (default: false).</p> <p>Example input file (<code>input.csv</code>): <pre><code>x,y\n1,2\n3,4\n</code></pre></p> <ul> <li>Input: A channel emitting CSV strings or files.</li> <li>Output: A channel emitting lists (rows) or maps (if header is specified).</li> <li>Arguments:<ul> <li><code>header</code> (optional): If true, parses the first row as column names and emits maps.</li> <li><code>skip</code> (optional): Number of lines to skip at the start.</li> </ul> </li> </ul> <pre><code>Channel.fromPath('input.csv')\n    .splitCsv(header: true)\n    .view() // Output: [x:1, y:2], [x:3, y:4]\n\nChannel.fromPath('input.csv')\n    .splitCsv(skip: 1)\n    .view() // Output: [1, 2], [3, 4]\n</code></pre> <p>Example (TSV file): Suppose you have a TSV file (<code>input.tsv</code>): <pre><code>x   y\n1   2\n3   4\n</code></pre> You can parse it using the <code>sep</code> argument: <pre><code>Channel.fromPath('input.tsv')\n    .splitCsv(header: true, sep: '\\t')\n    .view() // Output: [x:1, y:2], [x:3, y:4]\n</code></pre></p> <p>See also: <code>splitText</code>.</p>"},{"location":"Operators/#splitfasta","title":"splitFasta","text":"<p>Use <code>splitFasta</code> to parse FASTA files into sequence records or chunks.</p> <p>Example input file (<code>sample.fa</code>): <pre><code>&gt;seq1\nATGCATGC\n&gt;seq2\nGGCCTTAA\n</code></pre></p> <ul> <li>Input: A channel emitting FASTA files or strings.</li> <li>Output: A channel emitting sequence records (as maps) or text chunks.</li> <li>Arguments:<ul> <li><code>record</code> (optional): If true or a map, emits parsed records with specified fields.</li> </ul> </li> </ul> <p><pre><code>Channel.fromPath('sample.fa')\n    .splitFasta(record: [id: true, seqString: true])\n    .view { it.id + ': ' + it.seqString }\n</code></pre> See also: <code>splitFastq</code>.</p>"},{"location":"Operators/#splitfastq","title":"splitFastq","text":"<p>Use <code>splitFastq</code> to parse FASTQ files into sequence records or chunks.</p> <p>Example input file (<code>sample.fastq</code>): <pre><code>@seq1\nATGCATGC\n+\nIIIIIIII\n@seq2\nGGCCTTAA\n+\nJJJJJJJJ\n</code></pre></p> <ul> <li>Input: A channel emitting FASTQ files or strings.</li> <li>Output: A channel emitting sequence records (as maps) or text chunks.</li> <li>Arguments:<ul> <li><code>record</code> (optional): If true, emits parsed records.</li> </ul> </li> </ul> <p><pre><code>Channel.fromPath('sample.fastq')\n    .splitFastq(record: true)\n    .view { it.readHeader + ': ' + it.readString }\n</code></pre> See also: <code>splitFasta</code>.</p>"},{"location":"Operators/#splitjson","title":"splitJson","text":"<p>Use <code>splitJson</code> to parse JSON arrays or objects into individual records.</p> <p>Example input file (<code>input.json</code>): <pre><code>[1, 2, 3]\n</code></pre> or <pre><code>{\"a\": 1, \"b\": 2}\n</code></pre></p> <ul> <li>Input: A channel emitting JSON strings or files.</li> <li>Output: A channel emitting elements of arrays or key-value pairs of objects.</li> <li>Arguments: None.</li> </ul> <p><pre><code>Channel.fromPath('input.json')\n    .splitJson()\n    .view() // Output: 1, 2, 3\nChannel.fromPath('input.json')\n    .splitJson()\n    .view() // Output: [key:a, value:1], [key:b, value:2]\n</code></pre> See also: <code>splitCsv</code>.</p>"},{"location":"Operators/#file-and-data-persistence-operators","title":"File and Data Persistence Operators","text":"<p>These operators handle the collection and writing of data to files. Use them to persist results or intermediate data for later use or external analysis.</p>"},{"location":"Operators/#collectfile","title":"collectFile","text":"<p>Use <code>collectFile</code> to save all channel items to a file for reporting, archiving, or downstream tools.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A file containing all items, typically one per line or as specified.</li> <li>Arguments:<ul> <li><code>name</code> (required): Output file name.</li> <li><code>mode</code> (optional): File write mode (e.g., 'overwrite', 'append').</li> </ul> </li> </ul> <pre><code>Channel.of('foo', 'bar')\n    .collectFile(name: 'output.txt')\n// Writes 'foo' and 'bar' to output.txt\n</code></pre>"},{"location":"Operators/#save","title":"save","text":"<p>Use <code>save</code> to persist results or logs to a specific file location for reproducibility or sharing.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A file at the specified path containing all items.</li> <li>Arguments:<ul> <li><code>path</code> (required): Output file path.</li> </ul> </li> </ul> <pre><code>Channel.of(1, 2, 3)\n    .save('numbers.txt')\n// Writes 1, 2, 3 to numbers.txt\n</code></pre>"},{"location":"Operators/#mathematical-and-statistical-operators","title":"Mathematical and Statistical Operators","text":"<p>These operators perform mathematical computations on channel items. Use them to summarize, analyze, or reduce data streams.</p>"},{"location":"Operators/#count","title":"count","text":"<p>Use <code>count</code> to determine the size of a dataset or the number of results produced by a process.</p> <ul> <li>Input: A channel emitting any type of item.</li> <li>Output: A channel emitting a single integer representing the number of items received.</li> <li>Arguments: None.</li> </ul> <pre><code>Channel.of(1, 2, 3)\n    .count()\n    .view() // Output: 3\n</code></pre>"},{"location":"Operators/#sum","title":"sum","text":"<p>Use <code>sum</code> to calculate totals, such as the sum of measurements or scores.</p> <ul> <li>Input: A channel emitting numeric items.</li> <li>Output: A channel emitting a single value representing the sum of all items.</li> <li>Arguments: None.</li> </ul> <pre><code>Channel.of(1, 2, 3)\n    .sum()\n    .view() // Output: 6\n</code></pre>"},{"location":"Operators/#min","title":"min","text":"<p>Use <code>min</code> to find the smallest value in a dataset, such as the lowest score or measurement.</p> <ul> <li>Input: A channel emitting comparable items (e.g., numbers).</li> <li>Output: A channel emitting the minimum value among all items.</li> <li>Arguments: None.</li> </ul> <pre><code>Channel.of(5, 2, 8)\n    .min()\n    .view() // Output: 2\n</code></pre>"},{"location":"Operators/#max","title":"max","text":"<p>Use <code>max</code> to find the largest value in a dataset, such as the highest score or measurement.</p> <ul> <li>Input: A channel emitting comparable items (e.g., numbers).</li> <li>Output: A channel emitting the maximum value among all items.</li> <li>Arguments: None.</li> </ul> <pre><code>Channel.of(5, 2, 8)\n    .max()\n    .view() // Output: 8\n</code></pre>"},{"location":"Operators/#mean","title":"mean","text":"<p>Use <code>mean</code> to calculate the average of a set of values, such as mean coverage or expression.</p> <ul> <li>Input: A channel emitting numeric items.</li> <li>Output: A channel emitting the average (mean) value of all items.</li> <li>Arguments: None.</li> </ul> <pre><code>Channel.of(2, 4, 6)\n    .mean()\n    .view() // Output: 4.0\n</code></pre>"},{"location":"Operators/#testing-and-debugging-operators","title":"Testing and Debugging Operators","text":"<p>These operators assist in testing and debugging workflows.</p>"},{"location":"Operators/#view","title":"view","text":"<p>Use <code>view</code> to print items to the console for inspection. - Arguments:     - <code>closure</code> (optional): Custom formatting or logic for each item.</p> <pre><code>Channel.of('foo', 'bar')\n    .view() // Output: foo, bar\n</code></pre>"},{"location":"Operators/#subscribe","title":"subscribe","text":"<p>Use <code>subscribe</code> to perform actions on emitted items. - Arguments:     - <code>closure</code> (required): Action to perform for each item.</p> <pre><code>Channel.of(1, 2, 3)\n    .subscribe { println \"Item: $it\" }\n// Output: Item: 1\\nItem: 2\\nItem: 3\n</code></pre>"},{"location":"Outputs/","title":"Outputs","text":"<p>This page summarizes the different output types available for Nextflow processes. Use these declarations to specify how data is emitted from your process scripts.</p>"},{"location":"Outputs/#val","title":"val","text":"<p>Declare a variable output. The argument can be any value and can reference output variables defined in the process body.</p> <ul> <li>Syntax: <code>val(value)</code></li> <li>Output: Any value.</li> <li>Usage: <pre><code>process example {\n    output:\n    val(result)\n    script:\n    \"\"\"\n    result=42\n    echo \\$result\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Outputs/#path","title":"path","text":"<p>Declare a file output. Receives output files from the task environment matching the given pattern.</p> <ul> <li>Syntax: <code>path(pattern, [options])</code></li> <li>Output: File(s) matching the pattern.</li> <li>Options:<ul> <li><code>arity</code>: Number or range of expected files (e.g., <code>1</code>, <code>1..*</code>). Task fails if invalid.</li> <li><code>followLinks</code>: Return target files for symlinks (default: true).</li> <li><code>glob</code>: Interpret name as glob pattern (default: true).</li> <li><code>hidden</code>: Include hidden files (default: false).</li> <li><code>includeInputs</code>: Include input files matching the pattern (default: false).</li> <li><code>maxDepth</code>: Maximum directory levels to visit.</li> <li><code>type</code>: Type of paths returned: <code>file</code>, <code>dir</code>, or <code>any</code>.</li> </ul> </li> <li>Usage: <pre><code>process example {\n    output:\n    path('*.out', arity: 1)\n    script:\n    \"\"\"\n    echo foo &gt; result.out\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Outputs/#env","title":"env","text":"<p>Declare an environment variable output. Receives the value of the environment variable from the task environment.</p> <ul> <li>Syntax: <code>env(name)</code></li> <li>Output: String.</li> <li>Usage: <pre><code>process example {\n    output:\n    env(MY_VAR)\n    script:\n    \"\"\"\n    export MY_VAR=hello\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Outputs/#stdout","title":"stdout","text":"<p>Declare a stdout output. Receives the standard output of the task script.</p> <ul> <li>Syntax: <code>stdout</code></li> <li>Output: String (stdout).</li> <li>Usage: <pre><code>process example {\n    output:\n    stdout\n    script:\n    \"\"\"\n    echo Hello world\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Outputs/#eval","title":"eval","text":"<p>Declare an eval output. Receives the standard output of the given command, executed in the task environment after the task script.</p> <ul> <li>Syntax: <code>eval(command)</code></li> <li>Output: String (stdout of command).</li> <li>Usage: <pre><code>process example {\n    output:\n    eval('cat result.txt')\n    script:\n    \"\"\"\n    echo foo &gt; result.txt\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"Outputs/#tuple","title":"tuple","text":"<p>Declare a tuple output. Each argument should be an output declaration (<code>val</code>, <code>path</code>, <code>env</code>, <code>stdout</code>, or <code>eval</code>). Each tuple element is treated as a standalone output.</p> <ul> <li>Syntax: <code>tuple(arg1, arg2, ...)</code></li> <li>Output: Tuple with elements matching the declarations.</li> <li>Usage: <pre><code>process example {\n    output:\n    tuple(val(x), path(y))\n    script:\n    \"\"\"\n    echo foo &gt; $y\n    \"\"\"\n}\n</code></pre></li> </ul>"},{"location":"PipelineConfiguration/","title":"Process Configuration in Nextflow","text":"<p>A Nextflow configuration file (<code>nextflow.config</code>) allows you to control pipeline behavior and resource usage without modifying pipeline code. This is especially useful for adapting pipelines to different environments (local, cluster, cloud) or for tuning performance. For more details, see the official Nextflow configuration documentation.</p>"},{"location":"PipelineConfiguration/#basic-structure","title":"Basic Structure","text":"<p>A config file consists of assignments, blocks, and includes. Comments are denoted with <code>//</code>.</p> <pre><code>// Simple assignment\nworkDir = 'work'\nprocess.maxErrors = 10\n\n// Block syntax for grouping options\nprocess {\n    executor = 'sge'\n    queue = 'long'\n    memory = '8 GB'\n}\n</code></pre>"},{"location":"PipelineConfiguration/#process-scope","title":"Process Scope","text":"<p>The <code>process</code> scope is used to set defaults for all processes in your pipeline. You can also use selectors to target specific processes or labels.</p> <pre><code>process {\n    cpus = 4\n    memory = '8 GB'\n    withLabel: big_mem {\n        cpus = 16\n        memory = '64 GB'\n    }\n    withName: align_reads {\n        queue = 'short'\n    }\n}\n</code></pre> <ul> <li>Settings in the process definition override config defaults.</li> <li><code>withLabel</code> applies settings to processes with a given label.</li> <li><code>withName</code> applies settings to processes with a specific name.</li> </ul>"},{"location":"PipelineConfiguration/#profiles","title":"Profiles","text":"<p>Profiles allow you to define sets of configuration options for different environments. Select a profile at runtime with <code>-profile</code>.</p> <pre><code>profiles {\n    standard {\n        process.executor = 'local'\n    }\n    cluster {\n        process.executor = 'sge'\n        process.queue = 'long'\n    }\n}\n</code></pre> <p>Activate with: <pre><code>nextflow run main.nf -profile cluster\n</code></pre></p>"},{"location":"PipelineConfiguration/#configuring-profiles","title":"Configuring Profiles","text":"<p>Profiles are defined in the <code>profiles</code> block of your configuration file. Each profile is a named block containing configuration settings that override the defaults when the profile is activated. You can specify multiple profiles, and select one or more at runtime.</p> <p>Example:</p> <pre><code>profiles {\n    local {\n        process.executor = 'local'\n        docker.enabled = false\n    }\n    slurm {\n        process.executor = 'slurm'\n        process.queue = 'batch'\n        docker.enabled = true\n    }\n    test {\n        params.run_mode = 'test'\n        process.cpus = 1\n    }\n}\n</code></pre> <ul> <li>To activate a single profile:   <pre><code>nextflow run main.nf -profile slurm\n</code></pre></li> <li>To activate multiple profiles (settings are merged in order):   <pre><code>nextflow run main.nf -profile test,slurm\n</code></pre></li> </ul> <p>Notes: - Profile settings override the base configuration. - Use profiles to easily switch between local, cluster, or cloud environments, or to set up testing and production modes. - Avoid mixing dot and block syntax for the same scope within a profile to prevent unexpected overrides.</p> <p>For more details, see the official Nextflow configuration documentation.</p>"},{"location":"PipelineConfiguration/#slurm-profile-example","title":"Slurm Profile Example","text":"<p>The <code>slurm</code> profile is commonly used to configure Nextflow pipelines for execution on SLURM clusters. This profile sets the executor to <code>slurm</code> and allows you to specify SLURM-specific options such as queue/partition, account, and resource requests. See the <code>Environment &amp; Dependencies &gt; conda</code> section of Directives for more details on SLURM conda integration.</p> <p>Example:</p> <pre><code>profiles {\n    slurm {\n        process {\n            executor = 'slurm'\n            queue = 'standard'           // SLURM partition name\n            memory = '16 GB'\n            cpus = 4\n            time = '24h'\n            clusterOptions = ''\n        }\n    }\n}\n</code></pre> <ul> <li><code>process.executor</code>: Set to <code>slurm</code> to use the SLURM scheduler.</li> <li><code>process.queue</code>: Name of the SLURM partition (e.g., <code>batch</code>, <code>short</code>, <code>long</code>).</li> <li><code>process.memory</code>: Default memory request for all processes.</li> <li><code>process.cpus</code>: Default CPU request for all processes.</li> <li><code>process.time</code>: Maximum wall time for each process (e.g., <code>'2h'</code>, <code>'30m'</code>).</li> <li><code>process.clusterOptions</code>: Pass additional native SLURM options that would usually be submitted on the <code>sbatch</code> CLI.</li> </ul> <p>Usage:</p> <p>Activate the profile with: <pre><code>nextflow run main.nf -profile slurm\n</code></pre></p> <p>You can further customize the profile with additional SLURM options as needed. For more options, see the Process Directives documentation.</p>"},{"location":"PipelineConfiguration/#including-other-config-files","title":"Including Other Config Files","text":"<p>You can modularize configuration using <code>includeConfig</code>:</p> <pre><code>includeConfig 'path/to/cluster.config'\n</code></pre>"},{"location":"PipelineConfiguration/#useful-constants","title":"Useful Constants","text":"<ul> <li><code>projectDir</code>: Directory where the main script is located.</li> <li><code>launchDir</code>: Directory where the workflow was launched.</li> <li><code>workDir</code>: Directory for intermediate files.</li> </ul>"},{"location":"PipelineConfiguration/#conda-options","title":"Conda Options","text":"<p>Nextflow supports configuring Conda environments for process execution. The <code>conda</code> scope in your config file controls how Conda environments are created and managed.</p> <p>Common options include:</p> <pre><code>conda {\n    enabled = true                // Enable Conda support (default: false)\n    cacheDir = '/path/to/conda'   // Directory to store Conda environments\n    channels = ['bioconda','conda-forge'] // List of Conda channels\n    createOptions = '--override-channels' // Extra options for `conda create`\n    createTimeout = '30 min'      // Timeout for environment creation (default: 20 min)\n    useMamba = true               // Use mamba instead of conda (default: false)\n    useMicromamba = false         // Use micromamba (default: false)\n}\n</code></pre> <ul> <li><code>enabled</code>: Enables or disables Conda environments.</li> <li><code>cacheDir</code>: Path where Conda environments are stored (should be shared if using a cluster).</li> <li><code>channels</code>: List or comma-separated string of Conda channels to use.</li> <li><code>createOptions</code>: Additional command-line options for <code>conda create</code>.</li> <li><code>createTimeout</code>: Maximum time allowed for environment creation.</li> <li><code>useMamba</code>: Use mamba for faster environment creation.</li> <li><code>useMicromamba</code>: Use micromamba for lightweight environments.</li> </ul> <p>See the official documentation for more details and advanced usage.</p>"},{"location":"ProccessProperties/","title":"Process Properties","text":"<p>This page summarizes all built-in task properties available in Nextflow process bodies. These properties provide information about the current task execution and can be used in scripts, directives, and dynamic expressions.</p>"},{"location":"ProccessProperties/#task-identification","title":"Task Identification","text":"<p>Properties that uniquely identify the task or its context.</p>"},{"location":"ProccessProperties/#taskid","title":"task.id","text":"<p>The pipeline-level task index (corresponds to <code>task_id</code> in the execution trace).</p> <pre><code>process splitById {\n    input:\n    path sample\n    script:\n    \"\"\"\n    # Use task.id to create unique output folders for each pipeline task\n    mkdir output_${task.id}\n    cp $sample output_${task.id}/\n    \"\"\"\n}\n</code></pre>"},{"location":"ProccessProperties/#taskindex","title":"task.index","text":"<p>The process-level task index.</p> <pre><code>process nameFilesByIndex {\n    input:\n    path input_file\n    script:\n    \"\"\"\n    # Use task.index to generate sequentially named files\n    cp $input_file result_${task.index}.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"ProccessProperties/#taskname","title":"task.name","text":"<p>The current task name. Available only in <code>exec:</code> blocks.</p> <pre><code>process logTaskName {\n    exec:\n    // Use task.name to write a log entry for each task\n    new File(\"task_log.txt\") &lt;&lt; \"Started: ${task.name}\\n\"\n}\n</code></pre>"},{"location":"ProccessProperties/#taskprocess","title":"task.process","text":"<p>The current process name.</p> <pre><code>process addProcessNameToOutput {\n    script:\n    \"\"\"\n    # Add process name to output file for traceability\n    echo \"Generated by process: ${task.process}\" &gt; process_info.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"ProccessProperties/#taskhash","title":"task.hash","text":"<p>The unique hash ID for the task. Available only in <code>exec:</code> blocks.</p> <pre><code>process cacheByHash {\n    exec:\n    // Use task.hash to create a cache directory for each unique task\n    def cacheDir = new File(\"/tmp/cache/${task.hash}\")\n    cacheDir.mkdirs()\n    // ... store intermediate results in cacheDir ...\n}\n</code></pre>"},{"location":"ProccessProperties/#taskworkdir","title":"task.workDir","text":"<p>The unique working directory for the task. Available only in <code>exec:</code> blocks.</p> <pre><code>process saveLogToWorkDir {\n    exec:\n    // Save a log file in the task's work directory\n    new File(\"${task.workDir}/run.log\") &lt;&lt; \"Task started at ${new Date()}\\n\"\n}\n</code></pre>"},{"location":"ProccessProperties/#task-attempts-retry","title":"Task Attempts &amp; Retry","text":"<p>Properties related to task attempts and retry logic.</p>"},{"location":"ProccessProperties/#taskattempt","title":"task.attempt","text":"<p>The current attempt number for the task.</p> <pre><code>process retryDoubleResources {\n    errorStrategy 'retry'\n    maxRetries 4\n    cpus { 2 ** (task.attempt - 1) }\n    memory { (4 * (2 ** (task.attempt - 1))) + ' GB' }\n    script:\n    \"\"\"\n    # Double CPUs and memory on each retry attempt\n    echo \"Attempt: ${task.attempt}, CPUs: ${task.cpus}, Memory: ${task.memory}\"\n    ./run_analysis.sh\n    \"\"\"\n}\n</code></pre>"},{"location":"ProccessProperties/#taskpreviousexception","title":"task.previousException","text":"<p>New in version 24.10.0 The exception reported by the previous task attempt. Accessible only when retrying a failed task (<code>task.attempt &gt; 1</code>).</p> <pre><code>process retryWithException {\n    errorStrategy 'retry'\n    maxRetries 2\n    script:\n    \"\"\"\n    # If retrying, log the previous exception for debugging\n    if [[ ${task.attempt} -gt 1 ]]; then\n      echo \"Previous error: ${task.previousException}\" &gt;&gt; error.log\n    fi\n    ./run_analysis.sh\n    \"\"\"\n}\n</code></pre>"},{"location":"ProccessProperties/#taskprevioustrace","title":"task.previousTrace","text":"<p>New in version 24.10.0 The trace record associated with the previous task attempt. Accessible only when retrying a failed task (<code>task.attempt &gt; 1</code>). Useful for accessing previous runtime metrics.</p> <pre><code>process retryWithTrace {\n    errorStrategy 'retry'\n    maxRetries 2\n    script:\n    \"\"\"\n    # If retrying, adjust resources based on previous usage\n    if [[ ${task.attempt} -gt 1 ]]; then\n      echo \"Previous memory usage: ${task.previousTrace?.memory}\" &gt;&gt; resource.log\n      # Optionally, use this info to tune parameters\n    fi\n    ./run_analysis.sh\n    \"\"\"\n}\n</code></pre>"},{"location":"ProccessProperties/#task-execution-status","title":"Task Execution Status","text":"<p>Properties related to the execution result and directives.</p>"},{"location":"ProccessProperties/#taskexitstatus","title":"task.exitStatus","text":"<p>The exit code of the task script (only for <code>script:</code> or <code>shell:</code> blocks). Available after the script has executed.</p> <pre><code>process checkExit {\n    errorStrategy { task.exitStatus == 1 ? 'retry' : 'finish' }\n    script:\n    \"\"\"\n    # Simulate a command that may fail\n    ./run_step.sh || exit 1\n    \"\"\"\n}\n</code></pre>"}]}